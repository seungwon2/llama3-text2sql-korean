# llama3-text2sql-korean

## ğŸ”­ Overview

- ì´ repoì—ëŠ” Amazon Sagemakerë¥¼ ì´ìš©í•´ **llama3-8b ëª¨ë¸**ì„ **í•œêµ­ì–´ í˜¸í™˜ text to sql** ìš©ë„ë¡œ íŒŒì¸ íŠœë‹í•˜ëŠ” ì˜ˆì œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

- Amazon Bedrock Imported Modelsì— íŒŒì¸ íŠœë‹ì„ ì™„ë£Œí•œ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ì‹ë„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ê°œë°œ í™˜ê²½

- Amazon Sagemaker Notebook instance (ml.t3.medium)
- `conda_pytorch_p310` ì´ìš©
- Amazon Bedrock Imported Models ì´ìš© ì¶”ë¡ 

### ì‚¬ìš© ëª¨ë¸ ë° ë°ì´í„° ì…‹

- ì‚¬ìš© ëª¨ë¸: [Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
- ì‚¬ìš© ë°ì´í„° ì…‹: [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context)

#### ë°ì´í„° ì…‹ ì˜ˆì‹œ

```
Answer: SELECT COUNT(*) FROM head WHERE age > 56
Question: How many heads of the departments are older than 56 ?
Context: CREATE TABLE head (age INTEGER)
```

- Amazon Translate ì´ìš©í•˜ì—¬ user questionì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•œ í›„ ì‚¬ìš©

#### ë²ˆì—­ ë° ì „ì²˜ë¦¬ ì˜ˆì‹œ

```
{"messages":[{"content":"You are a powerful text-to-SQL model. Your job is to answer questions about a database.You can use the following table schema for context: CREATE TABLE table_name_52 (bronze VARCHAR, rank VARCHAR, total VARCHAR, nation VARCHAR)","role":"system"},{"content":"Return the SQL query that answers the following question: ì´ í•©ê³„ê°€ 3ë³´ë‹¤ ì‘ê³  í´ë€ë“œ êµ­ê°€ì´ê³  ìˆœìœ„ê°€ 4ë³´ë‹¤ í° ë¸Œë¡ ì¦ˆì˜ ì´ê³„ëŠ” ì–¼ë§ˆì…ë‹ˆê¹Œ?","role":"user"},{"content":"SELECT COUNT(bronze) FROM table_name_52 WHERE total < 3 AND nation = \"poland\" AND rank > 4","role":"assistant"}]}

```

## ğŸ—‚ï¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```text

.
â”œâ”€â”€ README.md
â”œâ”€â”€ datasets        # ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ full ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ ko_test_dataset.json
â”‚   â””â”€â”€ ko_train_dataset.json
â”œâ”€â”€ images          # ì‹¤ìŠµ ê°€ì´ë“œìš© ì´ë¯¸ì§€ ì…‹
â”‚
â”œâ”€â”€ notebook        # ì‹¤ìŠµ ìˆ˜í–‰ìš© íŒŒì´ì¬ ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ 0_setup.ipynb
â”‚   â”œâ”€â”€ 1-1_full_data_preprocessing.ipynb # í”„ë¡œë•ì…˜ ìš© full ë°ì´í„° ì…‹
â”‚   â”œâ”€â”€ 1_data_preprocessing.ipynb        # ì‹¤ìŠµìš© small sample ë°ì´í„° ì…‹
â”‚   â”œâ”€â”€ 2_fine_tuning.ipynb
â”‚   â”œâ”€â”€ 3_deploy.ipynb
â”‚   â””â”€â”€ 4_evaluation.ipynb
â””â”€â”€ scripts         # íŒŒì¸ íŠœë‹ì— í•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ run_fsdp_qlora_llama3.py

```

í”„ë¡œì íŠ¸ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- [0_setup.ipynb](notebook/0_setup.ipynb): ì´ˆê¸° í™˜ê²½ ì„¤ì • ê³¼ì •
- [1_data_preprocessing.ipynb](notebook/1_data_preprocessing.ipynb): ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì • (small sample ë°ì´í„° ì´ìš©, í”„ë¡œë•ì…˜ ìš© full ë°ì´í„° ì…‹ì€ [1-1_full_data_preprocessing.ipynb](notebook/1-1_full_data_preprocessing.ipynb) ë…¸íŠ¸ë¶ ì´ìš©)
- [2_fine_tuning.ipynb](notebook/2_fine_tuning.ipynb): ëª¨ë¸ íŒŒì¸ íŠœë‹ ê³¼ì •
- [3_deploy.ipynb](notebook/3_deploy.ipynb): Amazon Bedrockì— ëª¨ë¸ ë°°í¬ ê³¼ì •
- [4_evaluation.ipynb](notebook/4_evaluation.ipynb): ëª¨ë¸ í‰ê°€ ê³¼ì •

## ğŸ“ References

- [SageMaker ì—ì„œ Llama3.1 8B íŒŒì¸ íŠœë‹, ëª¨ë¸ ë°°í¬ ë° ì¶”ë¡  í•˜ê¸°](https://github.com/aws-samples/aws-ai-ml-workshop-kr/tree/master/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3-1)
- [Amazon Bedrock Samples](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/custom_models/import_models/llama-3/customized-text-to-sql-model.ipynb)
- [Import a fine-tuned Meta Llama 3 model for SQL query generation on Amazon Bedrock](https://aws.amazon.com/blogs/machine-learning/import-a-fine-tuned-meta-llama-3-model-for-sql-query-generation-on-amazon-bedrock/)
