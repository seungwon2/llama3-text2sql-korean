{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  04 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock의 fine tuning된 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "model_id = \"<ENTER_YOUR_MODEL_ARN_HERE>\"\n",
    "\n",
    "assert model_id != \"<ENTER_YOUR_MODEL_ARN_HERE>\", \"ERROR: Please enter your model id\"\n",
    "\n",
    "def get_sql_query(system_prompt, user_question):\n",
    "    \"\"\"\n",
    "    Generate a SQL query using Llama 3 8B\n",
    "    Remember to use the same template used in fine tuning\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"<s>[INST] <<SYS>>{system_prompt}<</SYS>>\\n\\n[INST]Human: Return the SQL query that answers the following question: {user_question}[/INST]\\n\\nAssistant:\"\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    response = client.invoke_model(modelId=model_id,\n",
    "                                   body=json.dumps(native_request))\n",
    "    response_text = json.loads(response.get('body').read())[\"outputs\"][0][\"text\"]\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터셋 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_json(\"../datasets/ko_test_dataset.json\", lines=True)[\"messages\"]\n",
    "\n",
    "def extract_content(dicts, role):\n",
    "    for d in dicts:\n",
    "        if d['role'] == role:\n",
    "            return d['content']\n",
    "    return None\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for role in ['system', 'user', 'assistant']:\n",
    "    df[role] = test_df.apply(lambda x: extract_content(x, role))\n",
    "del test_df\n",
    "\n",
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llama'] = df.apply(lambda row: get_sql_query(row['system'], row['user']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas로 데이터셋 미리보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude 3.5 Sonnet을 이용해 정확도 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function because Claude requires the Messages API\n",
    "\n",
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "my_config = Config(connect_timeout=60*3, read_timeout=60*3)\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)\n",
    "\n",
    "MAX_ATTEMPTS = 3 #how many times to retry if Claude is not working.\n",
    "\n",
    "def ask_claude(messages,system=\"\", model_version=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    \n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    promt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "    \n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(promt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else: #retry in 2 seconds\n",
    "                time.sleep(2)\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_score(system, user, assistant, llama):\n",
    "    db_schema = system[139:] # Remove generic instructions\n",
    "    question = user[58:] # Remove generic instructions\n",
    "    correct_answer = assistant\n",
    "    test_answer = llama\n",
    "    formatted_prompt = f\"\"\"You are a data science teacher that is introducing students to SQL. Consider the following question and schema:\n",
    "<question>{question}</question>\n",
    "<schema>{db_schema}</schema>\n",
    "    \n",
    "Here is the correct answer:\n",
    "<correct_answer>{correct_answer}</correct_answer>\n",
    "    \n",
    "Here is the student's answer:\n",
    "<student_answer>{test_answer}<student_answer>\n",
    "\n",
    "Please provide a numeric score from 0 to 100 on how well the student's answer matches the correct answer for this question.\n",
    "The score should be high if the answers say essentially the same thing.\n",
    "The score should be lower if some parts are missing, or if extra unnecessary parts have been included.\n",
    "The score should be 0 for an entirely wrong answer. Put the score in <SCORE> XML tags.\n",
    "Do not consider your own answer to the question, but instead score based only on the correct answer above.\n",
    "\"\"\"\n",
    "    _, result = ask_claude(formatted_prompt, model_version=\"sonnet\")\n",
    "    pattern = r'<SCORE>(.*?)</SCORE>'\n",
    "    match = re.search (pattern, result)\n",
    "    \n",
    "    return match.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for ix in range(len(df)):\n",
    "    response = float(get_score(df[\"system\"][ix], df[\"user\"][ix], df[\"assistant\"][ix], df[\"llama\"][ix]))\n",
    "    scores.append(response)\n",
    "print(\"Assigned scores: \", scores)\n",
    "average_score = sum(scores) / len(scores)\n",
    "print(\"Average score:\", average_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고: 테스트 결과 아래와 같이 90.45%의 평균 정확도를 보였습니다.\n",
    "    ```\n",
    "    Assigned scores:  [80.0, 90.0, 90.0, 95.0, 50.0, 40.0, 100.0, 90.0, 100.0, 75.0, 80.0, 100.0, 100.0, 80.0, 80.0, 100.0, 60.0, 100.0, 90.0, 100.0, 100.0, 100.0, 80.0, 60.0, 100.0, 100.0, 100.0, 95.0, 75.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.0, 75.0, 60.0, 95.0, 100.0, 100.0, 100.0, 100.0, 90.0, 40.0, 100.0, 0.0, 90.0, 100.0, 100.0, 100.0, 100.0, 75.0, 80.0, 100.0, 100.0, 100.0, 100.0, 60.0, 95.0, 100.0, 60.0, 100.0, 100.0, 100.0, 100.0, 100.0, 50.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 90.0, 100.0, 100.0, 100.0, 100.0, 100.0, 90.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 50.0, 100.0, 60.0, 100.0, 100.0, 100.0]\n",
    "    Average score: 90.45\n",
    "\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
