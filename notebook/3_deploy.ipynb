{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Deploy to Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock에 모델 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "\n",
    "![step1](../images/deploy_1.png)\n",
    "\n",
    "- Amazon Bedrock 콘솔에서 `Imported models`를 클릭합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 \n",
    "\n",
    "![step2](../images/deploy_2.png)\n",
    "\n",
    "- `Imported model`을 클릭하여 배포 작업을 시작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "![step3](../images/deploy_3.png)\n",
    "\n",
    "- `Model name`을 입력하고, 파인 튜닝한 모델을 저장한 S3 location을 선택한 후 `Import model`을 클릭합니다.\n",
    "- S3 경로는 `{S3버킷}/{training job 이름}/output/model/ `로 설정해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "![step4](../images/deploy_4.png)\n",
    "\n",
    "- Model import job은 llama-8b 기준 약 15분이 소요됩니다.\n",
    "- Model import job이 완료되면 `Models`에서 배포된 모델을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "![step5](../images/deploy_5.png)\n",
    "\n",
    "- 배포가 완료되면 `Bedrock Playground`에서 모델을 테스트해볼 수 있습니다. \n",
    "- 아래 sample question으로 시작해보세요.\n",
    "\n",
    "    ```\n",
    "    <s>[INST]<<SYS>>\n",
    "    \n",
    "    You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE table_name_11 (tournament VARCHAR)\n",
    "    <</SYS>>\n",
    "\n",
    "    [INST]\n",
    "    Human:\n",
    "    Return the SQL query that answers the following question: 1987년에 A가 있었던 대회는 어떤 대회입니까?\n",
    "\n",
    "    [/INST]\n",
    "    Assistant:\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock API로 응답 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amazon Bedrock의 invoke API로 llama3 모델을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = sess.boto_region_name\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "model_id = \"<ENTER_YOUR_MODEL_ARN_HERE>\"\n",
    "\n",
    "assert model_id != \"<ENTER_YOUR_MODEL_ARN_HERE>\", \"ERROR: Please enter your model id\"\n",
    "\n",
    "def get_sql_query(system_prompt, user_question):\n",
    "    \"\"\"\n",
    "    Generate a SQL query using Llama 3 8B\n",
    "    Remember to use the same template used in fine tuning\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"<s>[INST] <<SYS>>{system_prompt}<</SYS>>\\n\\n[INST]Human: Return the SQL query that answers the following question: {user_question}[/INST]\\n\\nAssistant:\"\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    response = client.invoke_model(modelId=model_id,\n",
    "                                   body=json.dumps(native_request))\n",
    "    response_text = json.loads(response.get('body').read())[\"outputs\"][0][\"text\"]\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set에 있는 질문 실행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 데이터셋을 저장할 리스트\n",
    "dataset = []\n",
    "\n",
    "# JSON 파일에서 데이터 읽기 (이 부분은 실제 데이터 로딩 방식에 따라 조정 필요)\n",
    "with open('../datasets/ko_test_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        dataset.append(data['messages'])\n",
    "\n",
    "# 무작위로 10개의 예제 선택 (실제 환경에서는 random.sample 등을 사용할 수 있습니다)\n",
    "sample_dataset = dataset[:10]\n",
    "\n",
    "# 각 데이터셋 항목에 대해 쿼리 생성 및 결과 비교\n",
    "for item in sample_dataset:\n",
    "    system_prompt = item[0]['content']\n",
    "    user_question = item[1]['content'].split(\": \", 1)[1]  # \"Return the SQL query that answers the following question: \" 부분 제거\n",
    "    assistant_answer = item[2]['content']\n",
    "    \n",
    "    # 모델을 사용하여 SQL 쿼리 생성\n",
    "    generated_query = get_sql_query(system_prompt, user_question)\n",
    "    \n",
    "    print(f\"질문: {user_question}\\n\")\n",
    "    print(f\"답변: {generated_query}\\n\")\n",
    "    print(f\"정답: {assistant_answer}\\n\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `user_question`에 한국어 질문을 직접 작성하여 실행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE table_name_11 (tournament VARCHAR)\"\n",
    "user_question = \"1987년에 A가 있었던 대회는 어떤 대회입니까?\"\n",
    "\n",
    "query = get_sql_query(system_prompt, user_question).strip()\n",
    "print(query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
